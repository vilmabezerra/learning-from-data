{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relatório de Inteligência Computacional 2\n",
    "\n",
    "## Aluna: Vilma Bezerra Alves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importando arquivos referentes aos homeworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from first_one import *\n",
    "from second_one import *\n",
    "from fifth_one import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note que este relatório tem títulos Homework 1, 2 e 5 de forma a se relacionar com a organização do código. Já o número das questões com seus respectivos ítens referem-se ao enunciado dos projetos passados por email."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ferramentas\n",
    "\n",
    "No desenvolvimento dos projetos que se seguem abaixo, foi-se utilizado a biblioteca *numpy* e a *math* para fazer os cálculos necessários entre vetores e matrizes, os quais são essenciais para a construção dos algoritmos aqui comentados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo de Aprendizagem Perceptron\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No pacote **algorithms** deste projeto, encontra-se o arquivo **perceptron_learning_algoritm.py**, onde foram construídas as principais funções para que seja executado o PLA. \n",
    "\n",
    ">Ao executar a função *run*, que tem como entrada os pontos de treino, a classificação destes pontos e os pesos inicias, os quais são inicializados randômicamente - como indicado pelo enunciado - caso não sejam passados como parâmetros, podemos ver o PLA iterando até encontrar um corte que melhor separe todos os pontos dados como entrada de acordo com suas respectivas classe. Esta função retorna os pesos finais, que representam a função *g*, e a quantidade de iterações para encontrá-los.\n",
    "\n",
    "Já no pacote **toys**, encontra-se o arquivo **first_one.py**, onde foram construídas funções para responder aos enunciados do Homework 1 relacionados ao PLA. Note que a função *generate_data_and_train_pla* foi desenvolvida especialmente com o propósito de responder aos enunciados que perguntam a média de iterações para se treinar o modelo e o erro fora da amostra quando treinando um modelo com N pontos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quantidade média de iterações e erro esperado para N = 10\n",
    "\n",
    "1. ítem b - Tendo em vista que chegamos a uma média de aproximadamente 13 iterações, 15 é o ítem que chega mais próximo.\n",
    "\n",
    "\n",
    "2. ítem c - Pois 0.18 é de mesma ordem de 0.1, valor apresentado neste ítem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took me an average of 12.976 iterations!\n",
      "Expected error: 0.18480000000000002\n"
     ]
    }
   ],
   "source": [
    "generate_data_and_train_pla(n_elements=10, min=-1, max=1, times_to_run=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quantidade média de iterações e erro esperado para N = 100\n",
    "\n",
    "3. ítem b - Já que tivemos uma média de 90 iterações, o valor mais próximo disso dentro os ítens seria 100.\n",
    "\n",
    "\n",
    "4. ítem b - Pois 0.025 é da mesma ordem de 0.01, valor apresentado neste ítem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took me an average of 90.168 iterations!\n",
      "Expected error: 0.02539\n"
     ]
    }
   ],
   "source": [
    "generate_data_and_train_pla(n_elements=100, min=-1, max=1, times_to_run=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressão Linear\n",
    "\n",
    "Também no pacote **algorithms** iremos encontrar o script **linear_regression.py**, onde foram desenvolvidas todas as funções necessárias para classificar dados treinando um modelo de regressão linear.\n",
    "\n",
    ">A função *run* desenvolvida neste script, tem como retorno os pesos encontrados e a matriz dos pontos. Os parâmetros de entrada são pontos gerados randomicamente com duas dimensões, porém tornou-se interessante retornar a matriz de pontos já que estes têm uma dimensão adicionada, a qual tem como objetivo representar o viés da função *g*.\n",
    "\n",
    ">Também nesse script temos a função *classify_points_given_weights*, que tem como entrada os pesos resultante de *run* e os pontos pontos utilizados para encontrar *g*. São retornadas as classificações que nossa regressão linear atribui a cada ponto, sendo possível assim, com o retorno dessa classificação calcular o erro dentro da amostra.\n",
    "\n",
    "No pacote **toys**, temos o arquivo **second_one.py** cujo desenvolvimento foi focado em responder as questões relacionadas ao Homework 2.\n",
    "\n",
    "A função *linear_regression_part1* em especial tem como objetivo responder as questões 5 e 6, que respectivamente pedem o erro médio dentro e o o erro médio fora da amostra levando em consideração N = 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Erro médio dentro e erro médio fora da amostra levando em consideração N = 100 para treino e outros N pontos para teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. ítem c - Como 0.04 está na mesma ordem de 0.01, podemos assumir tal ítem como resposta.\n",
    "\n",
    "\n",
    "6. ítem c - Como nosso erro fora da amostra foi por volta de 0.05, podemos assumir 0.01 como a melhor resposta dado o resultado obtido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average in sample error: 0.041\n",
      "Average out of sample error: 0.050100000000000006\n"
     ]
    }
   ],
   "source": [
    "linear_regression_part1(n_points=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quantidade média de iterações necessárias para o PLA encontrar *g* tendo como entrada pesos gerados pela Regressão Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Já a função *linear_regression_part2* se utiliza de funções também do script de PLA para averiguar a quantidade de iterações necessárias para se encontrar a função *g*, quando tem-se os pesos gerados pela regressão linear dados como entrada para o Perceptron. Foi bastante interessante observar que há uma redução considerável de , em média, 10 iteração com relação a ter os pesos sendo inicializados randomicamente como fizemos no Homework 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. ítem a - Tendo em vista que tivemos uma média de 4.5 iterações para que o Perceptron encontra-se uma função *g* que fizesse o corte apropriado entre os pontos, podemos considerar o ítem a uma boa resposta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average iterations: 4.505\n"
     ]
    }
   ],
   "source": [
    "linear_regression_part2(n_points=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressão Não-Linear\n",
    "\n",
    "Ainda utilizamos funções do script de **linear_regression.py** para analisar o treino com Regressão não-linear, já que para aplicá-la nós utilizaremos o mesmo modelo de Regressão Linear, porém dando como entrada dados com transformações não lineares. \n",
    "\n",
    "A função *non_linear_regression_part* encontrada no arquivo referente ao Homework 2, **second_one.py** , foi desenvolvida especialmente para a análise pedida nas questões 8, 9 e 10 do projeto, cujos enunciados pedem respectivamente e sempre utilizando a mesma função target *$f(x_1, x_2) = sign(x_1^2 + x_2^2 - 0.6)$* , o erro dentro da amostra fazendo Regressão Linear, os pesos e o erro fora da amostra utilizando Regressão Não-Linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Erro dentro da amostra utilizando Regressão Linear para o treino\n",
    "\n",
    "8. ítem b - Tivemos um erro médio dentro da amostra de 0.5 quando treinando o modelo com 1000 pontos no conjunto de treinamento e com 10% deles tendo sofrido ruído - ou seja, classe com sinal invertido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pesos médios quando utilizando Regressão Não-Linear para o treino \n",
    "\n",
    "9. ítem a - Tendo em vista que o peso tido como viés obtivemos como resultado -1.24 e os pesos para ambas as dimensões $x_1^2$ e $x_2^2$ obtivemos o valor 1.94, temos que o ítem a é o que mais se aproxima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Erro médio fora da amostra utilizando Regressão Não-Linear para o treino\n",
    "\n",
    "10. ítem b - 0.1 é o valor dentre os ítens que mais se aproxima do valor 0.12, obtido pelo algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear Regression average E_in when data is noisy: 0.505124\n",
      "\n",
      "\n",
      "Non-Linear Regression average weights: [-1.2412441935479055, 0.0006022355541053417, 0.001548300952127133, 0.0011981512348317284, 1.9485162042909776, 1.9479863014667902]\n",
      "\n",
      "\n",
      "Non-Linear Regression average E_out when data is noisy: 0.123119\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nonlinear_regression_part(n_points=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta seção do projeto foi interessante observar que o erro médio fora da amostra obtido pela Regressão Não-Linear foi menor do que o erro dentro da amostra obtido pela Regressão Linear. Isso nos trás fortes indícios de que pode ser melhor optar pela Regressão Não-Linear quando não existe uma solução aproximadamente linear, o que pode ser evidenciado pela acurácia ao se tentar utilizar modelos com cortes lineares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradiente Descendente\n",
    "\n",
    "No pacote **algorithms** deste projeto, encontra-se o arquivo **gradient_descent.py**, onde foram construídas as principais funções para que seja executado o Gradiente Descendente. Tal algoritmo tem como objetivo achar o mínimo local ótimo de uma função de erro, para isso tivemos como entrada o ponto inicial, a taxa de aprendizado e a função de erro a ser encontrado o mínimo definida na questão. Dada a função de erro $E(u, v) = (ue^v - 2ve^{-u})^2 $, calculou-se na mão a derivada parcial das duas variáveis presentes na função dada.\n",
    "\n",
    "##### Derivada parcial de $u$\n",
    "\n",
    "$2(ue^v - 2ve^{-u})(e^v + 2ve^{-u})$\n",
    "\n",
    "##### Derivada parcial de $v$\n",
    "\n",
    "$2(ue^v - 2ve^{-u})(ue^v - 2e^{-u})$\n",
    "\n",
    ">No arquivo **gradient_descent.py** temos duas funções essenciais para aplicação do conceito de Gradiente Descendente. Foram implementas a *gradient_descent_step* e a*coordinate_descent_step*. A primeira altera as variáveis das duas dimensões de acordo com suas respectivas derivadas parciais e a taxa de aprendizado definida. Já a segunda faz algo parecido porém alterando uma variável por vez, de modo a utilizar o novo valor de uma variável para calcular a derivada parcial da outra.\n",
    "\n",
    "A função *gradient_descent_part* desenvolvida no pacote **toys** internamente ao arquivo **fifth_one.py** tem como o objetivo apresentar as respostas para as perguntas 11, 12 e 13."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Iterações usando Gradiente Descendente para chegar ao erro de ordem $10^{-14}$\n",
    "\n",
    "11. ítem d - Tendo em vista que 10 é o valor mais próximo do valor obtido pelo algoritmo desenvolvido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Valor das coordenadas $u$ e $v$ ao chegar no erro de ordem $10^{-14}$\n",
    "\n",
    "12. ítem e - Já que os valores obtidos são de aproximadamente 0.45 e 0.24 respectivamente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ordem de erro obtida ao final de 15 iterações completas de Coordenada Descendente\n",
    "\n",
    "13. ítem a - O erro obtido ao final de 30 iterações (ou 15 iterações completas) foi na ordem de $10^{-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iterations to error get bellow to 1.0e-14: 11\n",
      "\n",
      "u and v coordinates at the end: (0.04473629039778207, 0.023958714099141746)\n",
      "\n",
      "Error after 15 full iterations: 1.398e-01\n"
     ]
    }
   ],
   "source": [
    "gradient_descent_part(min_value=1e-14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pode-se observar, então, que no caso desta função de erro o passo dado pelo Gradiente Descendente chegou mais rapidamento a um erro de ordem ainda menor do que o obtido pelo da Coordenada Descendente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressão Logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste algoritmo houve dificuldades em obter bons resultados. Já que para as 100 iterações, gerando pontos e suas classes randomicamente e analisando a diferença entre os pesos da época atual e a anterior até que tal diferença fosse menor que 0.01 como definido no enunciado, obteve-se um E_out médio muito fora do padrão, podendo-se dizer até incoerente. Como pode-se ver abaixo ao rodar a função *logistic_regression_part* desenvolvida no módulo **toys.fifth_one** ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Erro médio fora da amostra para treino com N = 100\n",
    "\n",
    "13. Nesta questão específica não pode-se identificar o ítem correto por meio do algoritmo desenvolvido. Acredita-se que houve erro na implementação da função de erro da Regressão Logística, o qual ainda não foi identificado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Média de épocas para convergir em um treino com N = 100\n",
    "\n",
    "14. ítem a - Tendo em vista que 350 é o valor mais próximo de 345, a qual foi a média de épocas que o algoritmo implementado levou para convergir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average epochs to converge: 345.49\n",
      "\n",
      "Average e_out to converge: 2.1323959907644707\n"
     ]
    }
   ],
   "source": [
    "logistic_regression_part(iterations=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialmente a Regressão Logística foi implementada neste projeto utilizando-se uma permutação não randômica entre os pontos. Ao mudar para a permutação randômica foi interessante notar o quão mais rápido convergiu, ou seja uma média menor de épocas para treinar a quantidade de pontos pedida devido somente a esta pequena mudança na implementação. O que deixa evidente a importância de ter noções de Probabilidade e Estatística ao se trabalhar treinando modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referências\n",
    "\n",
    "Para implementação deste projeto foram utilizados as aulas online Learning From Data disponibilizadas pela Instituto de Tecnologia da Califórnia (Caltech), bem como o livro Learning From Data cujo um dos autores é Yahser S. Abu-Mostafa."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
